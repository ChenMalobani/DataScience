{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "RNN names classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShirGrinblat/DataScience/blob/master/RNN_names_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiUQtPrPuETn",
        "colab_type": "text"
      },
      "source": [
        "# RNN - many to one \n",
        "many = name letters, one = language "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEvk1ta4ubA8",
        "colab_type": "code",
        "outputId": "6604a388-dee1-420b-8aa4-0266ba5ce51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq2tHTYjux-Y",
        "colab_type": "code",
        "outputId": "0a48ad0c-91e4-47e8-8d84-d01f77861f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install Unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLldOHV3uETq",
        "colab_type": "text"
      },
      "source": [
        "Load names from file, each file is a language \n",
        "constructing a dictionary in a form of:   {language: [names ...]}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8nJM8rhuETr",
        "colab_type": "code",
        "outputId": "4f7aa73f-746f-4444-fd56-85574ae701cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import unidecode\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "print(findFiles('/content/drive/My Drive/MusicProject/data/names/*.txt'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicodeToAscii(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return lines\n",
        "    #return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('/content/drive/My Drive/MusicProject/data/names/*.txt'):\n",
        "    category = filename.split('/')[-1].split('.')[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/MusicProject/data/names/Dutch.txt', '/content/drive/My Drive/MusicProject/data/names/Czech.txt', '/content/drive/My Drive/MusicProject/data/names/Greek.txt', '/content/drive/My Drive/MusicProject/data/names/Japanese.txt', '/content/drive/My Drive/MusicProject/data/names/Arabic.txt', '/content/drive/My Drive/MusicProject/data/names/Chinese.txt', '/content/drive/My Drive/MusicProject/data/names/Italian.txt', '/content/drive/My Drive/MusicProject/data/names/French.txt', '/content/drive/My Drive/MusicProject/data/names/German.txt', '/content/drive/My Drive/MusicProject/data/names/Irish.txt', '/content/drive/My Drive/MusicProject/data/names/Vietnamese.txt', '/content/drive/My Drive/MusicProject/data/names/Scottish.txt', '/content/drive/My Drive/MusicProject/data/names/Korean.txt', '/content/drive/My Drive/MusicProject/data/names/Russian.txt', '/content/drive/My Drive/MusicProject/data/names/Portuguese.txt', '/content/drive/My Drive/MusicProject/data/names/English.txt', '/content/drive/My Drive/MusicProject/data/names/Polish.txt', '/content/drive/My Drive/MusicProject/data/names/Spanish.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhxf9vsvuET2",
        "colab_type": "code",
        "outputId": "068e7cbd-fc1e-4b08-9f3f-814813ff232b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(category_lines['Italian'][:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GDJov_VyuET-",
        "colab_type": "code",
        "outputId": "9c56167e-1d13-403c-cf49-92800f04d935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    # print(letter)\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('J'))\n",
        "print(lineToTensor('Jones').size())\n",
        "# print(letterToIndex('G'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "torch.Size([5, 1, 57])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BihcAaWGuEUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return Variable(torch.zeros(1, self.hidden_size))\n",
        "\n",
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbbUOVvDuEUN",
        "colab_type": "text"
      },
      "source": [
        "We want to predict what is the origin of Albert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnuqDuTquEUO",
        "colab_type": "code",
        "outputId": "73253c31-6c57-400d-c38d-e245890bf48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "input = Variable(letterToTensor('A'))\n",
        "hidden = Variable(torch.zeros(1, n_hidden))\n",
        "\n",
        "output, next_hidden = rnn(input, hidden)\n",
        "\n",
        "input = Variable(letterToTensor('l'))\n",
        "output, next_hidden = rnn(input, next_hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBtofGHpuEUV",
        "colab_type": "code",
        "outputId": "e49c9e61-6f4b-4f65-d6d0-0a13389619fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "input = Variable(lineToTensor('Albert'))\n",
        "hidden = Variable(torch.zeros(1, n_hidden))\n",
        "\n",
        "output, next_hidden = rnn(input[0], hidden)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0544, 0.0622, 0.0566, 0.0498, 0.0548, 0.0512, 0.0523, 0.0553, 0.0562,\n",
            "         0.0526, 0.0584, 0.0605, 0.0573, 0.0519, 0.0565, 0.0571, 0.0563, 0.0566]],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp-HPMsguEUe",
        "colab_type": "text"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8gJAiBPuEUg",
        "colab_type": "code",
        "outputId": "7e62c554-ec9e-40cf-b060-59c71b5a9a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
        "    category_i = top_i[0][0]\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Czech', tensor(1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVkpcqjUuEUn",
        "colab_type": "code",
        "outputId": "4d6a7b7a-2a2e-4091-ce15-ddc3d23a1652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.LongTensor([all_categories.index(category)])\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "for i in range(10):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category = Polish / line = Kumiega\n",
            "category = English / line = Wolfe\n",
            "category = Japanese / line = Takekawa\n",
            "category = Greek / line = Close\n",
            "category = Greek / line = Matsoukis\n",
            "category = Irish / line = Macdermott\n",
            "category = Russian / line = Avalov\n",
            "category = Spanish / line = Fuentes\n",
            "category = Japanese / line = Sakiyurai\n",
            "category = English / line = Mackenzie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jAXwI28uEUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "# If you set this too high, it might explode. If too low, it might not learn\n",
        "learning_rate = 0.005\n",
        "\n",
        "# category_tensor: language, line_tensor: surname\n",
        "def train(category_tensor, line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    return output, loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJgy91tjuEU1",
        "colab_type": "code",
        "outputId": "de4fa117-1dc0-4cb8-d1de-0dbb238f522c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(category_tensor, line_tensor)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100,\n",
        "                                                timeSince(start), loss, line,\n",
        "                                                guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(current_loss / plot_every)\n",
        "        current_loss = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5000 5% (0m 7s) -0.0558 Koumans / Greek ✗ (Dutch)\n",
            "10000 10% (0m 14s) -0.0602 Lefévre / Scottish ✗ (French)\n",
            "15000 15% (0m 21s) -0.0614 Kelly / Portuguese ✗ (Scottish)\n",
            "20000 20% (0m 29s) -0.0572 Reynders / Greek ✗ (Dutch)\n",
            "25000 25% (0m 36s) -0.0612 Mertens / Greek ✗ (Dutch)\n",
            "30000 30% (0m 44s) -0.0616 Ślązak / Greek ✗ (Polish)\n",
            "35000 35% (0m 51s) -0.0625 Ramires / Greek ✗ (Portuguese)\n",
            "40000 40% (0m 58s) -0.0448 Chang / Greek ✗ (Korean)\n",
            "45000 45% (1m 5s) -0.0684 Hao / Italian ✗ (Chinese)\n",
            "50000 50% (1m 13s) -0.0448 Schmidt / Greek ✗ (German)\n",
            "55000 55% (1m 20s) -0.0378 Moulin / Italian ✗ (French)\n",
            "60000 60% (1m 27s) -0.0613 Ferro / Italian ✗ (Portuguese)\n",
            "65000 65% (1m 35s) -0.0492 Aswad / Chinese ✗ (Arabic)\n",
            "70000 70% (1m 42s) -0.6449 Zhu / Chinese ✓\n",
            "75000 75% (1m 49s) -0.0093 Iñíguez / Polish ✗ (Spanish)\n",
            "80000 80% (1m 57s) -0.0013 Róg / Chinese ✗ (Polish)\n",
            "85000 85% (2m 4s) -0.0031 Tong / Chinese ✗ (Vietnamese)\n",
            "90000 90% (2m 11s) -0.0039 Delgado / Italian ✗ (Portuguese)\n",
            "95000 95% (2m 19s) -0.0008 Reinders / Italian ✗ (Dutch)\n",
            "100000 100% (2m 26s) -0.9293 Bukoski / Polish ✓\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffO6PnO7uEU8",
        "colab_type": "code",
        "outputId": "5c020667-6118-49c2-fc82-5f98001743d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.show(all_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFFB489cuEVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "# Just return an output given a line\n",
        "def evaluate(line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClRGOHtE2JHM",
        "colab_type": "code",
        "outputId": "4d1abc34-d7a5-4535-c68f-458c6b7bc0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "predict('Miguel')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> Miguel\n",
            "(0.39) German\n",
            "(0.32) Arabic\n",
            "(0.09) Polish\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rPOS5nM2MP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}